{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcBsff4v62MmE4FXcf7R9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwalx/Tensorflow/blob/main/07_FoodVision_Big.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone project 1: Food Vision Big"
      ],
      "metadata": {
        "id": "96m0YLZeatYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU\n",
        "\n",
        "* Google colab offers free GPUs, however, not all of them are compatiable with mixed precision training\n",
        "\n",
        "Google colab offers:\n",
        "* K80 (not compatible)\n",
        "* P100 (not compatible)\n",
        "* Tesla T4 (compatible)\n",
        "\n",
        "Knowing this, in order to use mixed precision training we need access to a Tesla T4(from with Google colab) or if we're uisng our own hardware, our GPU needs a score of 7.0+"
      ],
      "metadata": {
        "id": "EEyaSpqkpGLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "2Zz2ecsqpGW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get helper functions\n",
        "\n",
        "In past module, we've created a bunch of helper functions to do small tasks required for our notebooks.\n",
        "\n",
        "Rather than rewriting all stuff, we can import a script and load them in from there.\n"
      ],
      "metadata": {
        "id": "gioTsZt6pGeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download helper functions script\n",
        "!wget = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\""
      ],
      "metadata": {
        "id": "N4lv8SXFpGhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import series of helher functions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from helper_functions import create_tensorboard_callback,plot_loss_curves,compare_historys"
      ],
      "metadata": {
        "id": "4lOt-6_BpGjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Tensorflow Datasets to download data\n"
      ],
      "metadata": {
        "id": "zqsFtPPWpGmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Tensorflow Datasets\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "metadata": {
        "id": "XsHoaHwHpGop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list all available datasets\n",
        "datasets_list=tfds.list_builders()\n",
        "print(\"food101\" in datasets_list)"
      ],
      "metadata": {
        "id": "dloUYPbspGrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data (will take some time)\n",
        "(train_data,test_data),ds_info=tfds.load(name=\"food101\",\n",
        "                                         split=[\"train\",\"validation\"],\n",
        "                                         shuffle_files=True,\n",
        "                                         as_supervised=True,   #data gets returned in tuple format(data,label)\n",
        "                                         with_info=True)"
      ],
      "metadata": {
        "id": "wGUR9-wZpGtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the food101 data from tensorflow Datasets\n",
        "\n",
        "To become one with our data, we want to find:\n",
        "* Class Names\n",
        "* The shape of our input data(image tensors)\n",
        "* The datatype of our input data\n",
        "* What the labels look like (eg: are they one-hot encoded)\n",
        "* Do labels match up with the class names"
      ],
      "metadata": {
        "id": "uO48NHosOrq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features of Food101 from TFDS\n",
        "ds_info.features"
      ],
      "metadata": {
        "id": "KflED2_QpGwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=ds_info.features[\"label\"].names\n",
        "class_names[:10]"
      ],
      "metadata": {
        "id": "HD05255FpGys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample of the train data\n",
        "train_one_sample=train_data.take(1)\n"
      ],
      "metadata": {
        "id": "56kPvZJwPnVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does one sample of our training data looks like\n",
        "train_one_sample"
      ],
      "metadata": {
        "id": "5my4-BIGpG1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output info about our training sample\n",
        "for image,label in train_one_sample:\n",
        "  print(f\"\"\"\n",
        "  Image shape:{image.shape}\n",
        "  Image datatype: {image.dtype}\n",
        "  Target class from Food101 (tensor form):{label})\n",
        "  Class name (str form):{class_names[label.numpy()]}\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "fcemtyX6pG4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#what does our image tensor from TFDS's Food101 look like\n",
        "image"
      ],
      "metadata": {
        "id": "gHI37j99pG7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the min and max values of TFDS's Food101\n",
        "tf.reduce_min(image),tf.reduce_max(image)"
      ],
      "metadata": {
        "id": "lqDd61S9pG9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot an image from Tensorflow datasets\n"
      ],
      "metadata": {
        "id": "7vb46kO0S_er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot an image tensor\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)\n",
        "plt.title(class_names[label.numpy()])\n",
        "plt.axis(False);   # add title to image to verify the label associated with the image"
      ],
      "metadata": {
        "id": "5i7UYHICS_hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create preprocessing function for our data\n",
        "\n",
        "Neural networks perfom best when data is in a certain way( eg: batched,normalized,etc).\n",
        "\n",
        "However, not all data (including data from Tensorflow Datasets) comes like this.\n",
        "\n",
        "So in order to get it ready for a neural network, you'll often have to write preprocessing fucntions and mpa it to your data.\n",
        "\n",
        "What we know about our data:\n",
        "* In'unit*' datatype\n",
        "* Comprised of all different size tenors (different sized images)\n",
        "* Not scaled(the pixel values are between 0 & 255)\n",
        "\n",
        "What we know models like:\n",
        "* data in 'float32' dtype (or for mixed precision 'float16' and 'float32')\n",
        "* For batch, Tensorflow likes all of the tensors within a batch to be the same size\n",
        "* Scaled(values between 0 & 1) also called normalized tensors generally perform better\n",
        "\n",
        "With these points in mind,we've got a few things we can tackle with a preprocessing function.\n",
        "\n",
        "since we're going to be using an EfficientNetBX pretrained model from tf.keras.applications we don't need to rescale our data(these architecture have recaling built-in).\n",
        "\n",
        "This means our functions need to:\n",
        "\n",
        "1. Reshape our images to all the same size\n",
        "2. Convert the dtype of our image tensors from `uint8` to `float32`"
      ],
      "metadata": {
        "id": "729E6L9pSRV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a function fro preprocessing images\n",
        "def preprocessing_img(image,label,img_shape=224):\n",
        "  \"\"\"\n",
        "  Convert image datatype from 'uint8' -> 'float32' and reshape image to \n",
        "  [imag_shape,img_shape,color_channel]\n",
        "\n",
        "  \"\"\"\n",
        "  image=tf.image.resize(image,[img_shape,img_shape])   #reshape to img_shape\n",
        "  return tf.cast(image,tf.float32),label  # return (float_32,label)"
      ],
      "metadata": {
        "id": "ZVYkZeywS_kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_img  =preprocessing_img(image,label)[0]\n",
        "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape:{image.shape},\\nDatatype:{image.dtype}\\n\")\n",
        "print(f\"Image after preprocessing:\\n {preprocessed_img[:2]} ..,\\n:Shape: {preprocessed_img.shape},\\nDatatype:{preprocessed_img.dtype}\")"
      ],
      "metadata": {
        "id": "NgqDTACbS_nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can still plot the image as long we divide by 255.(for matplotlib compatibility)\n",
        "plt.imshow(preprocessed_img/255.)\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "9vHWKuBgS_pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map preporcessing function to training data(and parallelize)\n",
        "train_data=train_data.map(map_func=preprocessing_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "#shuffle train data and turn them into batches and prefetch it(helps loading it faster)\n",
        "train_data=train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map preprocessing fuction to test data(and parallelize it)\n",
        "test_data=test_data.map(map_func=preprocessing_img,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Turn test data into batches and prefetch it(helps loading it faster)\n",
        "test_data=test_data.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vBR9ssWeS_sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data"
      ],
      "metadata": {
        "id": "1KbarDiWS_u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# create modelcheckpoint callback to save model;s progress\n",
        "checkpoint_path=\"model_checkpoint/cp.cpkt\"  # saving weights require \".cpkt\" extension\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                    monitor=\"val_accuracy\",\n",
        "                                                    save_best_only=True,\n",
        "                                                    save_weights_only=True,\n",
        "                                                    verbose=0)\n"
      ],
      "metadata": {
        "id": "mOv0wdotS_xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up mixed precision"
      ],
      "metadata": {
        "id": "LMn4w9StS_0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(policy=\"mixed_float16\")"
      ],
      "metadata": {
        "id": "gu8sXFfIS_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_precision.global_policy()"
      ],
      "metadata": {
        "id": "OaXCC3tfS_5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "input_shape=(224,224,3)\n",
        "base_model=tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable=False   #freeze the model\n",
        "\n",
        "inputs=tf.keras.layers.Input(shape=input_shape,name=\"input_shape\")\n",
        "\n",
        "# Since we are using EfficientNet we dont require rescaling it since it already contains rescaling layer in it\n",
        "# x=layers.Rescaling(1./255)(x)   \n",
        "\n",
        "x=base_model(inputs,training=False)\n",
        "x=layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "x=layers.Dense(len(class_names))(x)\n",
        "\n",
        "outputs=tf.keras.layers.Activation(\"softmax\",dtype=\"float32\",name=\"softmax_float32\")(x)\n",
        "model=tf.keras.Model(inputs,outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "3xw6IX6zS_8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bw3MPqp8awM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the dtype policies(are we using mixed precision)\n"
      ],
      "metadata": {
        "id": "N0pyRWzna9is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  print(layer.name,layer.trainable,layer.dtype,layer.dtype_policy)"
      ],
      "metadata": {
        "id": "O86hsMDSbLe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the feature extraction model with callbacks\n",
        "\n",
        "1. Build a feature extraction model,(general order of doing things is:)\n",
        "2. Fine-tuning some of its frozen layers"
      ],
      "metadata": {
        "id": "UXNqgSB6sVcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the feature extraction model with callbacks\n",
        "history_10_food_classes_feature_extract = model.fit(train_data,\n",
        "                                                    epochs=3, \n",
        "                                                    steps_per_epoch=(len(train_data)),\n",
        "                                                    validation_data=test_data,\n",
        "                                                    validation_steps=int(0.15 * len(test_data)),\n",
        "                                                    callbacks=[create_tensorboard_callback(dir_name=\"training_logs\",\n",
        "                                                                                           experiment_name=\"efficientnetb0_101_classes_feature_extraction\"),\n",
        "                                                               model_checkpoint]\n",
        "                                                    )"
      ],
      "metadata": {
        "id": "z5eDIXs8bLjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAaSTxcw4dfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLWTw2RlbLkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUlonwclbLmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttTg2e51bLoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpWW45-NbLrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}