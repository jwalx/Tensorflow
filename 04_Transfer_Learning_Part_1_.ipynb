{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4LKXCTc89DHduuNtlBGga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwalx/Tensorflow/blob/main/04_Transfer_Learning_Part_1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer learning with tensorflow part1: Feature extraction\n",
        "Transfer learning is leveraging a working model's existing and learned patterns for our own problem.\n",
        "\n",
        "There are two main benefits:\n",
        "1. Can leverage an existing neural network architecture proven to work on problems similiar to our own. \n",
        "2. Can leverage a working neural network architecture which has already learned patterns on similar data to our own, then we can adapt those patterns to our own data"
      ],
      "metadata": {
        "id": "payJ4uWfUlma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check whether we are running on gpu\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElNnvQj6Uy70",
        "outputId": "2a402343-84eb-4dbc-ff17-d7ba5399d99a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 10 11:49:52 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downlaod and becoming one with data\n",
        "import zipfile\n",
        "\n",
        "#download the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "#unzipping\n",
        "zip_ref=zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFHQOmvBUy-q",
        "outputId": "b8ba64f8-874f-4bb6-e80d-28d25b470857"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 11:49:52--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.153.128, 142.250.145.128, 173.194.69.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.153.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M  41.8MB/s    in 4.4s    \n",
            "\n",
            "2023-01-10 11:49:57 (36.2 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the number of dictectories\n",
        "import os\n",
        "\n",
        "# Walkthrough 10 percen data directory and list number of files\n",
        "for dirpath,dirnames,filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGJLof1oUzA-",
        "outputId": "1b6fc41e-2efb-4d96-e733-05f2ee60c19d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating data loaders(preparing the data)\n",
        "We'll use the ImageDataGenerator class to load the data in batches"
      ],
      "metadata": {
        "id": "LisK9YieUzDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setup data inputs\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE=(224,224)\n",
        "BATCH_SIZE=32\n",
        "\n",
        "train_dir  =\"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "train_datagen=ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen=ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "print(\"training images\")\n",
        "train_data_10_percent=train_datagen.flow_from_directory(train_dir,\n",
        "                                                        target_size=IMAGE_SHAPE,\n",
        "                                                        batch_size=BATCH_SIZE,\n",
        "                                                        class_mode=\"categorical\")\n",
        "print(\"testing images\")\n",
        "test_data_10_percent=test_datagen.flow_from_directory(test_dir,\n",
        "                                                      target_size=IMAGE_SHAPE,\n",
        "                                                      batch_size=BATCH_SIZE,\n",
        "                                                      class_mode=\"categorical\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljw5gtdHUzFy",
        "outputId": "9ef5d01c-c55a-450c-a663-ff24db4de4c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training images\n",
            "Found 750 images belonging to 10 classes.\n",
            "testing images\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up callbacks(things to run whilst our model trains)\n",
        "\n",
        "Callbacks are extra functionality you can add to your models to be peformed during or after training.Some of the most popular callbacks:\n",
        "\n",
        "* Tracking experiments with Tensorboard `callback`\n",
        "* Model checkpoint with the ModelCheckpoint `callback`\n",
        "* Stopping a model from training(before it trains too long and overfits) with the EarlyStopping `callback`"
      ],
      "metadata": {
        "id": "3ka8deswUzIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensorboard callback(functionized because we need to create a new one for each model)\n",
        "import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name,experiment_name):\n",
        "  log_dir = dir_name+\"/\"+ experiment_name+ \"/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving Tensorboard log files to:{log_dir}\")\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "AJUeV1q-UzKi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating models using Tensorflow Hub\n",
        "\n",
        "in past we've used Tensorflow to create our own model layer by layer from scratch.\n",
        "\n",
        "Now we're going to do a similiar process,except the majority of our model's layers are going to come from Tensorflow Hub.\n",
        "\n",
        "We can access pretrained model on TFhub"
      ],
      "metadata": {
        "id": "g3osoAt0UzNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets's compare the two models\n",
        "resnet_url=\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n",
        "\n",
        "efficientnet_url=\"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\""
      ],
      "metadata": {
        "id": "ttMaQFKkUzPr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies \n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "OEtQXVaSUzSP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and testing Resnet Tensorflow Hub feature Extraction model\n"
      ],
      "metadata": {
        "id": "n8zvQt5u6ptZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE+(3,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEwVPfIBUzUj",
        "outputId": "cf7377b0-f229-4be8-9b75-fc1377360760"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets make a create model() function to create a model from a URL\n",
        "def create_model(model_url,num_classes=10):\n",
        " \"\"\"\n",
        " Takes a Tensorflow Hub Url an creates a Keras Sequential model with it.\n",
        "\n",
        " Args:\n",
        " model_url(str): A tensorflow  hub feature extraction Url.\n",
        " num_classes(int): Number of output neurons in the output layer,\n",
        "                   should be equal to the number of target classess,default 10.\n",
        " \n",
        " Returns:\n",
        "    An uncompiled Keras Sequential model with model_url as feature extractor\n",
        "    layer and Dense output layer with num_classes output neurons.\n",
        " \"\"\"\n",
        " #download the pretrained model and save it as keras layer\n",
        " feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                          trainable=False,\n",
        "                                          name=\"feature_extraction_layer\",\n",
        "                                          input_shape=IMAGE_SHAPE+(3,))\n",
        " #create our own model\n",
        " model=tf.keras.Sequential([\n",
        "     feature_extractor_layer,\n",
        "     layers.Dense(num_classes,activation=\"softmax\",name=\"output_layer\")\n",
        " ])\n",
        " return model"
      ],
      "metadata": {
        "id": "dmXP3rfSUzW_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create Resnet model\n",
        "resnet_model = create_model(resnet_url,\n",
        "                            num_classes = train_data_10_percent.num_classes)"
      ],
      "metadata": {
        "id": "nZB_oV4dUzZS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                     optimizer =tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])\n",
        "history_resnet=resnet_model.fit(train_data_10_percent,\n",
        "                                epochs=10,\n",
        "                                steps_per_epoch=len(train_data_10_percent),\n",
        "                                validation_data=test_data_10_percent,\n",
        "                                validation_steps=len(train_data_10_percent),\n",
        "                                callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                                                       experiment_name=\"resnet50V2\")]\n",
        "                                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuGri-l6Vrih",
        "outputId": "b5ad8843-3c5d-4ce1-bf1c-2f3550b5b5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to:tensorflow_hub/resnet50V2/20230110-115014\n",
            "Epoch 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "TkZ0bvwXVrln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns seperate loss curves for accuracy and loss\n",
        "  \"\"\"\n",
        "  loss=history.history[\"loss\"]\n",
        "  val_loss=history.history[\"val_loss\"]\n",
        "\n",
        "  accuracy=history.history[\"accuracy\"]\n",
        "  val_accuracy=history.history[\"val_accuracy\"]\n",
        "\n",
        "  epochs=range(len(history.history[\"loss\"]))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.figure()\n",
        "  plt.plot(epochs,loss,label=\"training_loss\")\n",
        "  plt.plot(epochs,val_loss,label=\"val_loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs,accuracy,label=\"accuracy\")\n",
        "  plt.plot(epochs,val_accuracy,label=\"val_accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend();\n"
      ],
      "metadata": {
        "id": "AbonbX7gVroV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_resnet)"
      ],
      "metadata": {
        "id": "WhgxWh4XVrqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wow!!!\n",
        "\n",
        "That is incredible. Our transfer learning extractor model out performed all the previous models we built by hand....\n",
        "and in quicker training time and wuth only 10% percent of the training data"
      ],
      "metadata": {
        "id": "Z8CUX_DbVrtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and testing efficientnetB0 Tensorflow Hub feature Extraction model"
      ],
      "metadata": {
        "id": "ogkSwFYqVrv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create efficientnetB0 feature extarction layer\n",
        "efficientnet_model=create_model(model_url=efficientnet_url,\n",
        "                                num_classes=train_data_10_percent.num_classes)\n",
        "\n",
        "#compile\n",
        "efficientnet_model.compile(loss=\"categorical_crossentropy\",\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "#fit efficient net model to 10% data\n",
        "history_efficientnet=efficientnet_model.fit(train_data_10_percent,\n",
        "                                        epochs=5,\n",
        "                                        steps_per_epoch=len(train_data_10_percent),\n",
        "                                        validation_data=test_data_10_percent,\n",
        "                                        validation_steps=len(test_data_10_percent),\n",
        "                                        callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub/\",\n",
        "                                                                               experiment_name=\"efficientnetB0\")]\n",
        "                                        )"
      ],
      "metadata": {
        "id": "Ms0F_xQyVrya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_efficientnet)"
      ],
      "metadata": {
        "id": "5LT0Ay3M7OSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_model.summary()"
      ],
      "metadata": {
        "id": "wixsnHGe7OVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "Dz3FuUQ67OX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# different types of transfer learning \n",
        "\n",
        "* **\"As Is\"** transfer learning - using existing model with no changes what so ever(e.g using ImageNet model on 1000 Imagenet classes\n",
        "* **\"Feature Extraction\"** transfer learning- use the pretrained patterns of an existing model(eg:EfficientNETB0 trained on Imagenet) and adjust the output layer for your own problem(eg: 1000 classes > 10 classes of food)\n",
        "* **\"Fine tuning\"** - use the prelearned patterns of an existing model and fine-tune many or all of the underlying layers(including new output layers   "
      ],
      "metadata": {
        "id": "AfWc9NgE7OaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing our models results with tensorboard\n",
        "\n",
        "* note when you upload things to tensorboard.dev, your experiments are public. so if youre running private experiments(things you don't want others to see do not upload to tensorboard.dev"
      ],
      "metadata": {
        "id": "R-VQ3dJl7Ocn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#upload tensorboard dev records\n",
        "!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n",
        "  --name \"EfficientNetB0 VS ResNet50V2\" \\\n",
        "  --description \"Comparing these two TFHub model\" \\\n",
        "  --one_shot"
      ],
      "metadata": {
        "id": "GSIYykzV7Oe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y tensorboard experiments are uploaded publically here: https://tensorboard.dev/experiment/7RnrJTQBTQeLCEw21lWDAg/"
      ],
      "metadata": {
        "id": "2qwX0QHK7Ohu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0BAOGVZ7Okb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qY2EquXY7OnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nzizczW7Op_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOVsQLYv7Osb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9j4n9SxVr08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QW2ilWoMVr3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMJ8xip1Vr6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rSSw-nEVr8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}